import customtkinter as ctk
import tkinter as tk
from tkinter import Menu, messagebox
from PIL import Image, ImageTk

import cv2
import numpy as np
import tensorflow as tf
import mediapipe as mp
import pygame
import math
import threading
import time
from collections import deque
from tensorflow.keras.applications.resnet50 import preprocess_input

model = tf.keras.models.load_model(r"D:\DoAnTotNghiep\model_trainfull_ImageNet_22_chu_cai\best_model_00001_3 (1).keras")
#label_dict = {'D': 0, 'H': 1, 'U': 2, 'V': 3, 'nothing' : 4}
#reversed_dict = {value: key for key, value in label_dict.items()}

# Load model
#model = tf.keras.models.load_model(r"D:\DoAnTotNghiep\LearningRate10e-5\best_model_00001_2.keras")
label_dict = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'G': 5, 'H': 6, 'I': 7, 'K': 8, 'L': 9, 'M': 10, 'N': 11,
              'O': 12, 'P': 13, 'Q': 14, 'R': 15, 'S': 16, 'T': 17, 'U': 18, 'V': 19, 'X': 20, 'Y': 21}
reversed_dict = {v: k for k, v in label_dict.items()}

# MediaPipe
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.5, min_tracking_confidence=0.5)

# Kh·ªüi t·∫°o pygame
pygame.mixer.init()

# H√†m ƒë·ªÉ ph√°t √¢m gi·ªçng n√≥i 
def speak_cached(letter):
    if not sound_enabled:
        return
    filename = f"voices/{letter}.mp3"
    def play():
        pygame.mixer.music.load(filename)
        pygame.mixer.music.play()
    threading.Thread(target=play).start()


""" Thi·∫øt l·∫≠p giao di·ªán"""
# T·∫°o c·ª≠a s·ªï ch√≠nh v·ªõi CustomTkinter
window = ctk.CTk()
window.title("M√¥ h√¨nh Nh·∫≠n di·ªán Th·ªß ng·ªØ")
window.geometry("1200x800")
window.configure(fg_color="#ecf0f1")


ctk.set_appearance_mode("light")
ctk.set_default_color_theme("blue")
# ==== HEADER ====
header = ctk.CTkFrame(window, fg_color="#2c3e50", height= 120)
header.pack(side="top", fill="x")

# T·∫£i ·∫£nh logo PNG
logo_image = Image.open(r"C:\Users\ASUS\Downloads\logo-bach-khoa-dongphucsongphu2.png")
logo_ctk_image = ctk.CTkImage(light_image=logo_image, dark_image=logo_image, size=(100, 100))

# ==== Logo b√™n tr√°i ====
logo_label = ctk.CTkLabel(header, image=logo_ctk_image, text="")
logo_label.place(x=110, y=10)  # C√°ch tr√°i 20px, c√°ch tr√™n 30px cho ƒë·∫πp

# ==== Khung ch·ª©a ti√™u ƒë·ªÅ v√† slogan (Frame nh·ªè gi·ªØa header) ====
title_frame = ctk.CTkFrame(header, fg_color="transparent")  # transparent = trong su·ªët
#title_frame.place(relx=0.5, rely=0.5, anchor="center")  # CƒÉn gi·ªØa header



# ==== Ti√™u ƒë·ªÅ nh·ªè ====
slogan_label = ctk.CTkLabel(title_frame, text="ƒê·ªì √°n T·ªët nghi·ªáp",
                            font=("Roboto", 26), text_color="white")
slogan_label.pack(pady = 5)

# ==== Ti√™u ƒë·ªÅ ch√≠nh ====
title_label = ctk.CTkLabel(title_frame, text="ƒê·ªÅ t√†i: M√î H√åNH NH·∫¨N D·∫†NG TH·ª¶ NG·ªÆ",
                           font=("Arial", 26, "bold"), text_color="white")
title_label.pack(pady = 5)


# ==== MAIN AREA ====
main_frame = ctk.CTkFrame(window, fg_color="#f7f9fb")
main_frame.pack(fill="both", expand=True, padx=20, pady=10)

# Chia grid th√†nh 2 c·ªôt
main_frame.grid_columnconfigure(0, weight=10)  # Sidebar 10%
main_frame.grid_columnconfigure(1, weight=50)   # N·ªôi dung ch√≠nh 50%
main_frame.grid_rowconfigure(0, weight=1)      # M·ªôt d√≤ng duy nh·∫•t, full height



# ======= SIDEBAR =======   Khu v·ª±c ch·ª©a c√°c n√∫t ch·ª©c nƒÉng ch√≠nhch√≠nh
sidebar = ctk.CTkFrame(main_frame, fg_color="#ecf0f1", corner_radius=10)
sidebar.grid(row=0, column=0, sticky="nsew", padx=(10,5), pady=10)  # sticky="nsew" ƒë·ªÉ fill ƒë·ªß chi·ªÅu

button_font = ("Roboto", 16)
button_color = "#3498db"
# C√°c n√∫t b√™n tr√°i: M·ªü/T·∫Øt camera ‚Üí ch·ªçn ch·∫ø ƒë·ªô ‚Üí B·∫≠t/T·∫Øt √¢m thanh ‚Üí B·∫Øt ƒë·∫ßu/D·ª´ng ‚Üí Kh·ªüi ƒë·ªông l·∫°i ‚Üí Tho√°tTho√°t
btn_toggle_camera = ctk.CTkButton(sidebar, text="üìπ M·ªü Camera", font=button_font, fg_color= button_color, height=45, corner_radius=8, command=lambda: toggle_camera())
btn_toggle_camera.pack(pady=5, padx=8, fill="x")

btn_option = ctk.CTkButton(sidebar, text="üî† Ch·∫ø ƒë·ªô nh·∫≠n d·∫°ng", font=button_font, fg_color= button_color, height=45, corner_radius=8, command=lambda: option_detect())
btn_option.pack(pady=5, padx=8, fill="x")

btn_toggle_sound = ctk.CTkButton(sidebar, text="üîä B·∫≠t √¢m thanh",font=button_font, fg_color= button_color, height=45, corner_radius=8, command=lambda: toggle_sound())
btn_toggle_sound.pack(pady=5, padx=8, fill="x")

btn_toggle_recognize = ctk.CTkButton(sidebar, text="‚ñ∂ Start", font=button_font, fg_color="#27ae60", hover_color="#219150", height=45, corner_radius=8, command=lambda: toggle_tracking())
btn_toggle_recognize.pack(pady=5, padx=8, fill="x")

btn_reset = ctk.CTkButton(sidebar, text="üîÑ Reset", font=button_font, fg_color="#2c3e50", hover_color="#34495e", height=45, corner_radius=8, command=lambda: reset())
btn_reset.pack(pady=5, padx=8, fill="x")

exit_button = ctk.CTkButton(sidebar, text="‚ùå Tho√°t", font=button_font, fg_color="#e74c3c", hover_color="#c0392b", height=45, corner_radius=8, command=lambda: exit_window())
exit_button.pack(pady=5, padx=8, fill="x")

# ==== Ph·∫ßn hi·ªÉn th·ªã camera v√† k·∫øt qu·∫£ ====
center_frame = ctk.CTkFrame(main_frame, fg_color="white", corner_radius=10)
center_frame.grid(row=0, column=1, sticky="nsew", padx=(10,10), pady=5)

camera_frame = ctk.CTkFrame(center_frame,width=700, height=500, border_width=3, border_color="#3498db", corner_radius=10)
camera_frame.pack(pady=20, padx=20)

gray_background = np.full((480, 640, 3), (236, 240, 241), dtype=np.uint8)
empty_image_pil = Image.fromarray(gray_background)

# Chuy·ªÉn sang `CTkImage`
empty_image_ctk = ctk.CTkImage(light_image=empty_image_pil, size=(640, 480))

# Hi·ªÉn th·ªã video tr√™n giao di·ªán
video_label = ctk.CTkLabel(master=camera_frame, image=empty_image_ctk, text="", width=640, height=480)
video_label.image = empty_image_ctk  # Gi·ªØ tham chi·∫øu ƒë·ªÉ ·∫£nh kh√¥ng b·ªã x√≥a
video_label.pack()



# Nh√£n tr·∫°ng th√°i
status_label = ctk.CTkLabel(center_frame, text="", font=("Segoe UI", 20), text_color="#2c3e50")
status_label.pack(pady=5)

label_result = ctk.CTkLabel(center_frame, text="K·∫øt qu·∫£:",font=("Roboto",25,"bold"), anchor="w", text_color="#2c3e50")
label_result.pack(padx=20, anchor="w")
#label_result.pack_forget()

recognized_text = tk.StringVar(value="")
single_char_text = tk.StringVar()

#esult_textbox = ctk.CTkLabel(center_frame, textvariable=recognized_text, font=("Roboto",25), fg_color="#ecf0f1", text_color="black", corner_radius=8, anchor="w")
result_textbox = ctk.CTkLabel(center_frame, text="", font=("Roboto",25), fg_color="#ecf0f1", text_color="black", corner_radius=8, anchor="center")

result_textbox.pack(padx=20, pady=(0, 10), fill="x")

# ==== BUTTONS UNDER RESULT ====
buttons_frame = ctk.CTkFrame(center_frame, fg_color="white")
buttons_frame.pack(pady=5, padx=0, anchor="center")

btn_clear_all = ctk.CTkButton(buttons_frame, text="üóë X√≥a to√†n b·ªô", font=button_font, fg_color= "#ecf0f1", text_color="black", command= lambda: clear_all_text())
#btn_clear_all.pack(side="left", padx=10, fill = "x")
btn_clear_all.pack_forget()

btn_space = ctk.CTkButton(buttons_frame, text="‚ê£ Space", font=button_font,fg_color= "#ecf0f1", text_color="black",  command=lambda: add_space())
#btn_space.pack(side="left", padx=10, fill = "x")
btn_space.pack_forget()

btn_delete_one = ctk.CTkButton(buttons_frame, text="‚å´ X√≥a ", font=button_font, fg_color= "#ecf0f1", text_color="black", command= lambda: delete_last_character())
#btn_delete_one.pack(side="left", padx=10, fill = "x")
btn_delete_one.pack_forget()

# ==== FOOTER ====
footer = ctk.CTkLabel(center_frame, text="¬© 2025 | Version 1.0 ¬∑ Dam Yen Nhi", font=("Roboto",13), text_color="#7f8c8d")
footer.place(relx=1, rely=1, anchor="se")  # 'sw' l√† "south-west" = g√≥c d∆∞·ªõi b√™n tr√°i

def align_title_to_camera():
    window.update_idletasks()

    # L·∫•y v·ªã tr√≠ khung camera
    camera_x = camera_frame.winfo_rootx()
    camera_width = camera_frame.winfo_width()
    camera_center = camera_x + camera_width // 2

    # L·∫•y v·ªã tr√≠ g·ªëc c·ªßa window
    window_x = window.winfo_rootx()

    # T√≠nh to·∫° ƒë·ªô x c·ªßa ti√™u ƒë·ªÅ trong c·ª≠a s·ªï
    new_title_x = camera_center - window_x

    # C·∫≠p nh·∫≠t v·ªã tr√≠ title_frame cho cƒÉn gi·ªØa v·ªõi camera
    title_frame.place(x=new_title_x, rely=0.5, anchor="center")

# G·ªçi sau khi giao di·ªán render
window.after(200, align_title_to_camera)


""" Khai b√°o bi·∫øn """

last_detected_time = time.time() # ƒê·∫øm th·ªùi gian kh√¥ng ph√°t hi·ªán k√≠ t·ª± tay
hand_was_present = True 
has_detected = False  # ƒê√£ t·ª´ng nh·∫≠n di·ªán ƒë∆∞·ª£c ch·ªØ c√°i hay ch∆∞a


cap = None
is_camera_on = False
is_recognizing = False
tracking_active = False
imgSize = 224
frame_for_prediction = None
prediction_buffer = deque(maxlen=3)
last_spoken_char = ""
last_speak_time = 0
sound_enabled = False  # B·∫≠t √¢m thanh m·∫∑c ƒë·ªãnh
speak_delay = 2
space_added = False
recognition_mode = None  # Bi·∫øn ch·ªçn ch·∫ø ƒë·ªô
tracking_option = False





""" H√†m x·ª≠ l√Ω ·∫£nh ƒë·∫ßu v√†o t·ª´ webcam"""
def preprocess_image(imCrop):
    imWhite = np.ones((imgSize, imgSize, 3), np.uint8) * 255
    h_crop, w_crop = imCrop.shape[:2]
    aspectRatio = h_crop / w_crop

    if aspectRatio > 1:
        k = imgSize / h_crop
        wCal = math.ceil(k * w_crop)
        imgResize = cv2.resize(imCrop, (wCal, imgSize))
        w_gap = (imgSize - wCal) // 2
        imWhite[:, w_gap:w_gap + wCal] = imgResize
    else:
        k = imgSize / w_crop
        hCal = math.ceil(k * w_crop)
        imgResize = cv2.resize(imCrop, (imgSize, hCal))
        h_gap = (imgSize - hCal) // 2
        imWhite[h_gap:h_gap + hCal, :] = imgResize

    img_input = cv2.cvtColor(imWhite, cv2.COLOR_BGR2RGB)
    img_input = cv2.resize(img_input, (224, 224))
    #img_input = img_input.astype(np.float32) / 255.0
    img_input = np.expand_dims(img_input, axis=0)
    img_input = preprocess_input(img_input)
    return img_input
    

""" H√†m nh·∫≠n di·ªán """
def prediction_worker():
    global frame_for_prediction, prediction_buffer, last_spoken_char, last_speak_time
    global last_detected_time, recognized_text, has_detected
    while True:
        if frame_for_prediction is not None:
            try:
                img_input = preprocess_image(frame_for_prediction)
                prediction = model.predict(img_input)
                confidence = np.max(prediction)
                predicted_label = np.argmax(prediction)
                print(predicted_label)
                print(confidence)

                if confidence >= 0.9:
                    predicted_char1 = reversed_dict[predicted_label]
                    if predicted_char1 == "nothing":
                        predicted_char = ""
                    else:
                         predicted_char = predicted_char1
                    prediction_buffer.append(predicted_char)
                    has_detected = True 

                    if len(prediction_buffer) == prediction_buffer.maxlen and all(c == predicted_char for c in prediction_buffer):
                        if recognition_mode == "sequence": # ch·ªçn ch·∫ø ƒë·ªô hi·ªÉn th·ªã chu·ªói
                            if time.time() - last_detected_time > 1.5:
                                recognized_text.set(recognized_text.get() + predicted_char)
                                last_detected_time = time.time()
                               
                        if recognition_mode == "letter":
                            single_char_text.set(f"Ch·ªØ {predicted_char}")        
                            current_time = time.time()
                            #if predicted_char != last_spoken_char and current_time - last_speak_time >= speak_delay:
                            if current_time - last_speak_time >= speak_delay:    
                                speak_cached(predicted_char)
                                last_spoken_char = predicted_char
                                last_speak_time = current_time
                                    
                            #update_result(f"Ch·ªØ: {predicted_char}", "#ecf0f1")
                            

                        
                else:
                    prediction_buffer.clear()
                    single_char_text.set("")
                    #update_result("", "Kh√¥ng ph·∫£i ch·ªØ c√°i", "red")

            except Exception as e:
                print("L·ªói trong lu·ªìng d·ª± ƒëo√°n:", e)
            frame_for_prediction = None
        time.sleep(0.05)

""" H√†m x·ª≠ l√Ω khung h√¨nh t·ª´ webcam """  
def update_frame():
    global frame_for_prediction
    global last_detected_time
    global space_added, recognized_text, has_detected
    if not is_camera_on:
        return

    ret, frame = cap.read()
    if not ret:
        return
    
    frame = cv2.flip(frame, 1)
    display_frame = frame.copy()
    img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = hands.process(img_rgb)

    #if is_recognizing and results.multi_hand_landmarks:
    if tracking_active:
        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                h, w, _ = frame.shape
                x_min = w
                y_min = h
                x_max = 0
                y_max = 0
                for lm in hand_landmarks.landmark:
                    x, y = int(lm.x * w), int(lm.y * h)
                    x_min = min(x_min, x)
                    y_min = min(y_min, y)
                    x_max = max(x_max, x)
                    y_max = max(y_max, y)

                if y_max - y_min > 0 and x_max - x_min > 0:
                    imCrop = frame[y_min-20:y_max+20, x_min-20:x_max+20]
                    cv2.rectangle(display_frame, (x_min-20, y_min-20), (x_max+20, y_max+20), (0, 255, 0), 2)
                    frame_for_prediction = imCrop.copy()
        else:
            if recognition_mode == "letter":
                single_char_text.set("")


    img_rgb = cv2.cvtColor(display_frame, cv2.COLOR_BGR2RGB)

    # Chuy·ªÉn ƒë·ªïi ·∫£nh t·ª´ NumPy sang PIL
    img_pil = Image.fromarray(img_rgb)
    img_ctk = ctk.CTkImage(light_image=img_pil, size=(640, 480))

    # C·∫≠p nh·∫≠t ·∫£nh m·ªõi v√†o label (KH√îNG t·∫°o label m·ªõi!)
    video_label.configure(image=img_ctk)
    video_label.image = img_ctk  # Gi·ªØ tham chi·∫øu ·∫£nh, tr√°nh b·ªã x√≥a

    window.after(10, update_frame)


# X√°c nh·∫≠n x√≥a to√†n b·ªô
def clear_all_text():
    confirm = messagebox.askyesno("X√°c nh·∫≠n", "B·∫°n c√≥ ch·∫Øc ch·∫Øn mu·ªën x√≥a to√†n b·ªô n·ªôi dung kh√¥ng?")
    if confirm:
        recognized_text.set("")

# X√≥a 1 k√Ω t·ª± cu·ªëi
def delete_last_character():
    current_text = recognized_text.get()
    if current_text:
        recognized_text.set(current_text[:-1])


# H√†m hi·ªÉn th·ªã th√¥ng b√°o ng·∫Øn
def show_simple_message(message, duration=2000):  # duration t√≠nh b·∫±ng ms
    notification_label.configure(text=message)
    notification_label.place(relx=0.5, rely=0.9, anchor="center")
    window.after(duration, lambda: notification_label.place_forget())

# Trong ph·∫ßn setup giao di·ªán, b·∫°n t·∫°o 1 Label ·∫©n
notification_label = ctk.CTkLabel(window, text="", font=("Segoe UI", 14), text_color="red")
        


""" H√†m cho n√∫t nh·∫•n"""
#N√∫t b·∫≠t t·∫Øt √¢m thanh
def toggle_camera():
    global cap, is_camera_on, tracking_active, result_textbox
    if is_camera_on:
        # N√∫t ƒëang chuy·ªÉn t·ª´ tr·∫°ng th√°i b·∫≠t sang t·∫Øt camera
        if cap:
            cap.release()
        cap = None
        is_camera_on = False
        #tracking_active = False
        #btn_toggle_recognize.configure(text="‚ñ∂Start", text_color="#27ae60")
        # C·∫≠p nh·∫≠t giao di·ªán v·ªõi CustomTkinter
        btn_toggle_camera.configure(text="üìπM·ªü Camera", fg_color= button_color)
        #status_label.configure(text = "ƒê√£ t·∫Øt camera", text_color = "red")
        video_label.configure(image=empty_image_ctk)  # D√πng `CTkImage`
        if tracking_active:
            messagebox.showwarning("Th√¥ng b√°o", "H√£y m·ªü camera ƒë·ªÉ ti·∫øp t·ª•c nh·∫≠n di·ªán")
        
    else:
        # N√∫t chuy·ªÉn t·ª´ tr·∫°ng th√°i t·∫Øt sang b·∫≠t camera
        cap = cv2.VideoCapture(0)
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        if not cap.isOpened():
            messagebox.showerror("L·ªói", "Kh√¥ng th·ªÉ m·ªü camera")
            return
        
        is_camera_on = True
        btn_toggle_camera.configure(text="T·∫Øt Camera", fg_color="#2c3e50")
        #status_label.configure(text = "Camera ƒë√£ ƒë∆∞·ª£c m·ªü", text_color = "#2c3e50")
        update_frame()

# N√∫t ch·ªçn ch·∫ø ƒë·ªô
def option_detect():
    show_popup()


def show_popup():

    def on_select(option):
        global recognition_mode, result_textbox
        if option == "Nh·∫≠n di·ªán t·ª´ng ch·ªØ c√°i":
            recognition_mode = "letter"
            status_label.configure(text= "ƒêang ·ªü ch·∫ø ƒë·ªô nh·∫≠n di·ªán t·ª´ng ch·ªØ c√°i", text_color = "#2c3e50")
            result_textbox.configure(textvariable = single_char_text)
            recognized_text.set("")
            
            btn_clear_all.pack_forget()
            btn_space.pack_forget()
            btn_delete_one.pack_forget()
        if option == "Nh·∫≠n di·ªán chu·ªói k√Ω t·ª±":
            recognition_mode = "sequence"
            status_label.configure(text= "ƒêang ·ªü ch·∫ø ƒë·ªô nh·∫≠n di·ªán chu·ªói k√Ω t·ª±", text_color = "#2c3e50")
            result_textbox.configure(textvariable=recognized_text)
            btn_clear_all.pack(side="left", padx=10, fill = "x")
            btn_space.pack(side="left", padx=10, fill = "x")
            btn_delete_one.pack(side="left", padx=10, fill = "x")
        print(f"Ch·∫ø ƒë·ªô ƒëang ch·ªçn: {recognition_mode}")
        popup.destroy()
    

    # T·∫°o c·ª≠a s·ªï popup
    popup = ctk.CTkToplevel(window)
    popup.attributes("-topmost", True)
    popup.geometry("350x200")
    popup.title("Ch·ªçn ch·∫ø ƒë·ªô")
    popup.configure(fg_color="#ffffff")

    # Ti√™u ƒë·ªÅ
    label_title = ctk.CTkLabel(popup, text="Ch·ªçn ch·∫ø ƒë·ªô:", font=("Segoe UI", 16, "bold"), text_color="#2c3e50")
    label_title.pack(pady=10)

    # Danh s√°ch t√πy ch·ªçn
    options = ["Nh·∫≠n di·ªán t·ª´ng ch·ªØ c√°i", "Nh·∫≠n di·ªán chu·ªói k√Ω t·ª±"]
    colors = ["#3498db", "#27ae60"]

    for i, option in enumerate(options):
        btn = ctk.CTkButton(popup, text=option, font=("Segoe UI", 14), corner_radius=25,
                            fg_color=colors[i], hover_color="#34495e",
                            text_color="white", width=180, height=40,
                            command=lambda opt=option: on_select(opt))
        btn.pack(pady=5)        

#N√∫t b·∫≠t t·∫Øt √¢m thanh
def toggle_sound():
    global sound_enabled
    global is_camera_on, tracking_active
    if sound_enabled:
        # N√∫t ƒëang ·ªü tr·∫°ng th√°i t·∫Øt √¢m thanh
        sound_enabled = False
        btn_toggle_sound.configure(text="üîä B·∫≠t √¢m thanh")

    else:
        # N√∫t ƒëang ·ªü tr·∫°ng th√°i b·∫≠t √¢m
        if recognition_mode:
            if not is_camera_on:
                messagebox.showinfo("Th√¥ng b√°o", "Vui l√≤ng m·ªü camera")
            else:
                if not tracking_active:
                    messagebox.showinfo("Th√¥ng b√°o", "Vui l√≤ng nh·∫•n n√∫t Start")
                else:
                    sound_enabled = True
                    btn_toggle_sound.configure(text="üîá T·∫Øt √¢m thanh", fg_color="#2c3e50")
        else:
            if not is_camera_on:
                messagebox.showinfo("Th√¥ng b√°o", "Vui l√≤ng m·ªü camera v√† ch·ªçn ch·∫ø ƒë·ªô")
            else:
                if not tracking_active:
                    messagebox.showinfo("Th√¥ng b√°o", "Vui l√≤ng ch·ªçn ch·∫ø ƒë·ªô")

#N√∫t Start/Stop
def toggle_tracking():
    global tracking_active, notification_label
    global is_camera_on, recognition_mode
    if tracking_active:
        # N√∫t ƒëang chuy·ªÉn t·ª´  tr·∫°ng th√°i d·ª´ng nh·∫≠n di·ªán sang nh·∫≠n di·ªán
        confirm = messagebox.askyesno("X√°c nh·∫≠n", "B·∫°n c√≥ ch·∫Øc ch·∫Øn mu·ªën d·ª´ng nh·∫≠n di·ªán kh√¥ng?")
        if confirm:
            tracking_active = False

            # C·∫≠p nh·∫≠t giao di·ªán v·ªõi CustomTkinter
            btn_toggle_recognize.configure(text="‚ñ∂Start", fg_color = "#27ae60" )    

    else:
        # N√∫t ƒëang chuy·ªÉn t·ª´ tr·∫°ng th√°i nh·∫≠n di·ªán 
        if not is_camera_on:
            if recognition_mode:
                messagebox.showinfo("Th√¥ng b√°o", "Vui l√≤ng m·ªü camera")
            else:  
                messagebox.showinfo("Th√¥ng b√°o", "Vui l√≤ng m·ªü camera v√† ch·ªçn ch·∫ø ƒë·ªô")
    
        else:
            if recognition_mode:
                tracking_active = True

                btn_toggle_recognize.configure(text="‚èπ Stop", fg_color="#e74c3c")
            else:
                messagebox.showwarning("Th√¥ng b√°o", "Vui l√≤ng ch·ªçn ch·∫ø ƒë·ªô")                    

#N√∫t Reset
def reset():
    global is_camera_on, sound_enabled, tracking_active, recognition_mode, recognized_text, result_textbox
    global cap

    if is_camera_on == True:
        if cap:
            cap.release()
        cap = None
        is_camera_on = False
    video_label.configure(image=empty_image_ctk)  # Hi·ªÉn th·ªã ·∫£nh tr·∫Øng       
    is_camera_on = False
    tracking_active = False
    recognition_mode = None
    sound_enabled = False
    btn_clear_all.pack_forget()
    btn_space.pack_forget()
    btn_delete_one.pack_forget()
    btn_toggle_camera.configure(text="üìπM·ªü Camera")
    btn_toggle_recognize.configure(text="‚ñ∂ Start")
    btn_toggle_sound.configure(text="üîä B·∫≠t √¢m thanh")
    recognized_text.set("")
    single_char_text.set("")
    status_label.configure(text = "")

# N√∫t tho√°t
def exit_window():
    global is_camera_on, sound_enabled, tracking_active, recognition_mode, recognized_text, result_textbox
    global cap

    if is_camera_on:
        if cap is not None:
            cap.release()
        cap = None
        is_camera_on = False

    video_label.configure(image=empty_image_ctk)      
    tracking_active = False
    recognition_mode = None
    sound_enabled = False

    btn_clear_all.pack_forget()
    btn_space.pack_forget()
    btn_delete_one.pack_forget()

    recognized_text.set("")
    single_char_text.set("")
    status_label.configure(text="")

    window.destroy()

# N√∫t th√™m kho·∫£ng tr·∫Øng
def add_space():
    global space_added, last_detected_time, recognized_text, has_detected
    if has_detected:
        #if space_added == True: 
        recognized_text.set(recognized_text.get() + " ")
        last_detected_time = time.time() 
        space_added == False
        has_detected = False

threading.Thread(target=prediction_worker, daemon=True).start()
window.mainloop()
if cap:
    cap.release()
cv2.destroyAllWindows()

